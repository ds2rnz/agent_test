import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.tools import tool
from datetime import datetime

import pytz
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from dotenv import load_dotenv
import os

from typing import TypedDict, Annotated, List
from langgraph.graph import StateGraph, END
from langchain_community.document_loaders import PyPDFLoader
 #from langchain.text_splitter import RecursiveCharacterTextSplitter
import tempfile
import ast

# from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings
# from langchain_chroma import Chroma
# from langchain.agents import PromptTemplate
from langchain_community.vectorstores import FAISS
from openai import OpenAI

import concurrent.futures
import traceback
import inspect
import time
from langchain.messages import AIMessage

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI()

llm = ChatOpenAI(
    model="gpt-5",
    temperature=0.4,
    timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    max_retries=2 ) 

# -- ìƒíƒœ íƒ€ì… ì •ì˜ --
# class GraphState(TypedDict):
#     messages: Annotated[list, object]
#     pdf_path: str
#     pdf_content: str
#     chunks: List[str]
#     analysis_result: str



def debug_wrap(func):
    """í•¨ìˆ˜ ì‹¤í–‰ ì‹œ ì—ëŸ¬ë‚˜ ì¤‘ë‹¨ì ì„ ì¶”ì í•˜ê¸° ìœ„í•œ ë””ë²„ê¹… ë˜í¼"""
    def wrapper(*args, **kwargs):
        func_name = func.__name__
        try:
            print(f"[DEBUG] â–¶ ì‹¤í–‰ ì‹œì‘: {func_name}")
            result = func(*args, **kwargs)
            print(f"[DEBUG] âœ… ì‹¤í–‰ ì„±ê³µ: {func_name}")
            return result
        except Exception as e:
            tb = traceback.format_exc()
            print(f"\n[ERROR] âŒ í•¨ìˆ˜ '{func_name}' ì—ì„œ ì˜ˆì™¸ ë°œìƒ:")
            print(f"  â””â”€ {e}")
            print(tb)
            st.error(f"âŒ í•¨ìˆ˜ '{func_name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.code(tb, language="python")
            raise
    return wrapper


# -- PDF ì²˜ë¦¬ í•¨ìˆ˜ë“¤ --
# def load_pdf_node(state: GraphState) -> GraphState:
#     pdf_path = state.get("pdf_path", "")
#     if not pdf_path or not os.path.exists(pdf_path):
#         return {
#             "pdf_content": "",
#             "messages": [AIMessage(content=f"íŒŒì¼ì´ ì—†ê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {pdf_path}")]
#         }
#     try:
#         loader = PyPDFLoader(pdf_path)
#         pages = loader.load()
#         full_text = "\n\n".join([page.page_content for page in pages])
#         return {
#             "pdf_content": full_text,
#             "messages": [AIMessage(content=f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(pages)}í˜ì´ì§€, {len(full_text):,}ì")]
#         }
#     except Exception as e:
#         return {
#             "pdf_content": "",
#             "messages": [AIMessage(content=f"âŒ PDF ë¡œë“œ ì‹¤íŒ¨: {str(e)}")]
#         }

# def load_pdf_node(state: GraphState) -> GraphState:
#     """PDF íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë…¸ë“œ"""
#     pdf_path = state.get("pdf_path", "")
    
#     if not pdf_path:
#         return {
#             "pdf_content": "",
#             "messages": [AIMessage(content="PDF ê²½ë¡œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")]
#         }
    
#     if not os.path.exists(pdf_path):
#         return {
#             "pdf_content": "",
#             "messages": [AIMessage(content=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")]
#         }
    
#     try:
#         loader = PyPDFLoader(pdf_path)
#         pages = loader.load()
#         full_text = "\n\n".join([page.page_content for page in pages])
        
#         return {
#             "pdf_content": full_text,
#             "messages": [AIMessage(content=f"âœ… PDF ë¡œë“œ ì™„ë£Œ: {len(pages)}í˜ì´ì§€, {len(full_text):,}ì")]
#         }
#     except Exception as e:
#         return {
#             "pdf_content": "",
#             "messages": [AIMessage(content=f"âŒ PDF ë¡œë“œ ì‹¤íŒ¨: {str(e)}")]
#         }


# def chunk_pdf_node(state: GraphState) -> GraphState:
#     content = state.get("pdf_content", "")
#     if not content:
#         return {"chunks": [], "messages": [AIMessage(content="ë¶„í• í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")]}
#     try:
#         splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
#         chunks = splitter.split_text(content)
#         return {"chunks": chunks, "messages": [AIMessage(content=f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")]}
#     except Exception as e:
#         return {"chunks": [], "messages": [AIMessage(content=f"âŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {str(e)}")]}

# def chunk_pdf_node(state: GraphState) -> GraphState:
#     """PDF ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ë…¸ë“œ"""
#     content = state.get("pdf_content", "")
    
#     if not content:
#         return {"chunks": [],
#             "messages": [AIMessage(content="ë¶„í• í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")]
#         }
    
#     try:
#         text_splitter = RecursiveCharacterTextSplitter(
#             chunk_size=500,
#             chunk_overlap=50,
#             length_function=len,
#         )
#         chunks = text_splitter.split_text(content)
        
#         return {"chunks": chunks,
#             "messages": [AIMessage(content=f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")]
#         }
#     except Exception as e:
#         return {"chunks": [],
#             "messages": [AIMessage(content=f"âŒ í…ìŠ¤íŠ¸ ë¶„í•  ì‹¤íŒ¨: {str(e)}")]
#         }



# def analyze_pdf_node(state: GraphState) -> GraphState:
#     content = state.get("pdf_content", "")
#     chunks = state.get("chunks", [])
#     if not content:
#         return {"analysis_result": "", "messages": [AIMessage(content="ë¶„ì„í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")]}
    # try:
    #     words = [w for w in content.lower().split() if len(w) > 3]
    #     word_freq = {}
    #     for w in words:
    #         word_freq[w] = word_freq.get(w, 0) + 1
    #     top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]

    #     analysis = {
    #         "word_count": len(words),
    #         "char_count": len(content),
    #         "chunk_count": len(chunks),
    #         "keywords": top_keywords,
    #         "preview": content[:500]
    #     }
    #     return {
    #         "analysis_result": str(analysis),
    #         "messages": [AIMessage(content="âœ… PDF ë¶„ì„ ì™„ë£Œ")]
    #     }
    # except Exception as e:
    #     return {"analysis_result": "", "messages": [AIMessage(content=f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")]}
 
#     try:
#         word_count = len(content.split())
#         char_count = len(content)
#         chunk_count = len(chunks)
        
#         # í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹ˆë„ ê¸°ë°˜)
#         words = content.lower().split()
#         word_freq = {}
#         for word in words:
#             if len(word) > 3:
#                 word_freq[word] = word_freq.get(word, 0) + 1
        
#         top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
#         analysis = {
#             "word_count": word_count,
#             "char_count": char_count,
#             "chunk_count": chunk_count,
#             "keywords": top_keywords,
#             "preview": content[:500]
#         }
        
#         return {
#             "analysis_result": str(analysis),
#             "messages": [AIMessage(content="âœ… PDF ë¶„ì„ ì™„ë£Œ")]
#         }
#     except Exception as e:
#         return {
#             "analysis_result": "",
#             "messages": [AIMessage(content=f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")]
#         }



# @st.cache_resource
# def create_pdf_analysis_graph():
#     """PDF ë¶„ì„ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
#     graph = StateGraph(GraphState)
#     graph.add_node("load_pdf", load_pdf_node)
#     graph.add_node("chunk_pdf", chunk_pdf_node)
#     graph.add_node("analyze_pdf", analyze_pdf_node)
#     graph.set_entry_point("load_pdf")
#     graph.add_edge("load_pdf", "chunk_pdf")
#     graph.add_edge("chunk_pdf", "analyze_pdf")
#     graph.add_edge("analyze_pdf", END)
#     return graph.compile()



# -- ë„êµ¬ ì •ì˜ --
@tool
def get_current_time(timezone: str, location: str) -> str:
    """í˜„ì¬ ì‹œê°„ì„ ì§€ì •ëœ íƒ€ì„ì¡´ê³¼ ìœ„ì¹˜ì— ë§ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import pytz
    from datetime import datetime
    try:
        tz = pytz.timezone(timezone)
        now = datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
        return f'{timezone} ({location}) í˜„ì¬ì‹œê° {now}'
    except pytz.UnknownTimeZoneError:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì„ì¡´: {timezone}"

@tool
def get_web_search(query: str, search_period: str) -> str:
    """DuckDuckGo APIë¥¼ ì´ìš©í•´ ì§€ì •ëœ ê¸°ê°„ ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    wrapper = DuckDuckGoSearchAPIWrapper(region="kr-kr", time=search_period)
    search = DuckDuckGoSearchResults(api_wrapper=wrapper, source="news", results_separator=';\n')
    return search.invoke(query)

tools = [get_current_time, get_web_search]
tool_dict = {tool.name: tool for tool in tools}
llm_with_tools = llm.bind_tools(tools)


@debug_wrap
def get_ai_response(messages):
    response = llm_with_tools.stream(messages)
    gathered = None
    for chunk in response:
        yield chunk
        if gathered is None:
            gathered = chunk
        else:
            gathered += chunk

    if gathered and getattr(gathered, "tool_calls", None):
        st.session_state.messages.append(gathered)
        for tool_call in gathered.tool_calls:
            selected_tool = tool_dict.get(tool_call['name'])
            if selected_tool:
                with st.spinner("ë„êµ¬ ì‹¤í–‰ ì¤‘..."):
                    tool_msg = selected_tool.invoke(tool_call)
                    st.session_state.messages.append(tool_msg)
        # ë„êµ¬ í˜¸ì¶œ í›„ ì¬ê·€ì ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        yield from get_ai_response(st.session_state.messages)


# @debug_wrap
# def answer_question(query: str, timeout_sec: int = 60):
#     """LLM ê¸°ë°˜ PDF QA - ThreadExecutor ì œê±°í•œ ì•ˆì •ì ì¸ ë²„ì „"""

#     st.write("ğŸš€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘")
#     start_time = time.time()

#     vectorstore = st.session_state.get("vectorstore")
#     if vectorstore is None:
#         st.warning("âš ï¸ PDF í•™ìŠµì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
#         return "ë¨¼ì € PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  í•™ìŠµì‹œì¼œ ì£¼ì„¸ìš”."

#     st.write("âœ… vectorstore í™•ì¸ ì™„ë£Œ")

#     try:
#         # ë¬¸ì„œì—ì„œ ìœ ì‚¬ë„ ê²€ì‚¬
#         docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)
        
#         st.write(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ íšŸìˆ˜: {len(docs_with_scores)}ê°œ")
        
#         # ë””ë²„ê¹…: ìœ ì‚¬ë„ ì ìˆ˜ í‘œì‹œ
#         for i, (doc, score) in enumerate(docs_with_scores, 1):
#             st.write(f"  ë¬¸ì„œ {i} ìœ ì‚¬ë„: {score:.4f}")
        
#         # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
#         SIMILARITY_THRESHOLD = 1 
        
#         relevant_docs = [doc for doc, score in docs_with_scores if score < SIMILARITY_THRESHOLD]
        
#         if not relevant_docs:
#             st.warning(f"âš ï¸ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ ìœ ì‚¬ë„: {min(score for _, score in docs_with_scores):.4f})")
#             return "ì£„ì†¡í•©ë‹ˆë‹¤. "
        
#         st.success(f"âœ… {len(relevant_docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

#         # ==================== Retriever ìƒì„± ====================
#         retriever = vectorstore.as_retriever(
#             search_type="similarity", 
#             search_kwargs={"k": 3}
#         )
#         st.write("âœ… retriever ìƒì„± ì™„ë£Œ")

       
#         # ==================== QA Chain ìƒì„± ====================
#         qa_chain = RetrievalQA.from_chain_type(
#             llm=llm,  # llm ê°€ì ¸ì˜¤ê¸°
#             chain_type="stuff",
#             retriever=retriever,
#             return_source_documents=True,
#             )
#         st.write("âœ… qa_chain ìƒì„± ì™„ë£Œ")

#         # ì§ˆë¬¸ ì‹¤í–‰
#         try:
#             with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
#                 result = qa_chain.invoke({"query": query})
#         except Exception as e:
#             st.error(f"âŒ invoke() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             st.code(traceback.format_exc(), language="python")
#             return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
#         elapsed = time.time() - start_time
#         st.success(f"âœ… ì‘ë‹µ ì™„ë£Œ ({elapsed:.2f}ì´ˆ)")

#         # ê²°ê³¼ ì¶”ì¶œ
#         if isinstance(result, dict):
#             answer = result.get("result", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
#             # LLMì´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•œ ê²½ìš° ê°ì§€
#             if "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in answer or "ê´€ë ¨ì´ ì—†" in answer:
#                 st.info("ğŸ’¡ í•™ìŠµëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì´ ê´€ë ¨ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
            
#             # ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ (ì„ íƒì‚¬í•­)
#             if result.get("source_documents"):
#                 with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
#                     for i, doc in enumerate(result["source_documents"], 1):
#                         st.text_area(f"ë¬¸ì„œ {i}", doc.page_content[:300], height=200)
            
#             return answer
#         else:
#             return str(result)

#     except Exception as e:
#         st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         st.code(traceback.format_exc(), language="python")
#         return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    


# @debug_wrap
# def process1_f(uploaded_files1):
#     if uploaded_files1 and len(uploaded_files1) > 3:
#         st.warning("PDFëŠ” ìµœëŒ€ 3ê°œê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤!")
#         st.warning("PDFíŒŒì¼ì„ 3ê°œë§Œ ì„ íƒí•˜ì—¬ ì£¼ì„¸ìš”!")
#         return None
#         uploaded_files1 = uploaded_files1[:3]

#     if not uploaded_files1:
#         return None

#     with st.spinner("PDF ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘... ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
#         all_splits = []
#         for uploaded_file in uploaded_files1:
#             with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
#                 tmp_file.write(uploaded_file.read())
#                 tmp_path = tmp_file.name

#             loader = PyPDFLoader(tmp_path)
#             data = loader.load()
#             splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
#             splits = splitter.split_documents(data)
#             all_splits.extend(splits)

#             # íŒŒì¼ ë‹«ê³  ì‚­ì œ
#             tmp_file.close()
#             os.remove(tmp_path)

#         embedding = OpenAIEmbeddings(model="text-embedding-3-large", api_key=OPENAI_API_KEY)
#         persist_directory = "c:/faiss_store"
#         os.makedirs(persist_directory, exist_ok=True)
#         st.write(f"ì´ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(all_splits)}")

#         # ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
#         batch_size = 20
#         vectorstore = None
#         for i in range(0, len(all_splits), batch_size):
#             batch = all_splits[i:i+batch_size]
#             st.write(f"ë°°ì¹˜ {i//batch_size + 1} ì„ë² ë”© ì¤‘...")
#             # batch ë¬¸ì„œ ì„ë² ë”©
#             try:
#                 if vectorstore is None:
#                     vectorstore = FAISS.from_documents(batch, embedding
#                         # documents=batch,
#                         # embedding=embedding,
#                         # persist_directory=persist_directory
#                     )
#                 else:
#                     vectorstore.add_documents(batch)
#                 vectorstore.save_local(persist_directory)
#                 time.sleep(1.5)  # API ê³¼ë¶€í•˜ ë°©ì§€ìš© ì•½ê°„ì˜ ëŒ€ê¸°
#             except Exception as e:
#                 st.error(f"ì„ë² ë”© ì—ëŸ¬: {e}")    

#         st.success("í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
#         return vectorstore



# @debug_wrap
# def process2_f(uploaded_files2):
#     if not uploaded_files2:
#         st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”!")
#         return

#     with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
#         tmp_file.write(uploaded_files2.read())
#         tmp_path = tmp_file.name

#     try:
#         with st.spinner("ğŸ“„ PDF ë¶„ì„ ì¤‘..."):
#             app = create_pdf_analysis_graph()
#             initial_state = {
#                 "messages": [],
#                 "pdf_path": tmp_path,
#                 "pdf_content": "",
#                 "chunks": [],
#                 "analysis_result": ""
#             }
#             progress_bar = st.progress(0)
#             status_text = st.empty()

#             status_text.text("ğŸ“„ PDF ë¡œë”© ì¤‘...")
#             progress_bar.progress(33)
#             result = app.invoke(initial_state)
#             progress_bar.progress(100)
#             status_text.text("âœ… ë¶„ì„ ì™„ë£Œ!")

#             st.success("âœ… PDF ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

#             # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
#             analysis_data = ast.literal_eval(result.get("analysis_result", "{}"))

#             tab1, tab2, tab3 = st.tabs(["ğŸ“Š ë¶„ì„ ê²°ê³¼", "ğŸ“ í‚¤ì›Œë“œ", "ğŸ” ë¯¸ë¦¬ë³´ê¸°"])
#             with tab1:
#                 col1, col2, col3 = st.columns(3)
#                 col1.metric("ì´ ë‹¨ì–´ ìˆ˜", f"{analysis_data.get('word_count', 0):,}")
#                 col2.metric("ì´ ë¬¸ì ìˆ˜", f"{analysis_data.get('char_count', 0):,}")
#                 col3.metric("ì²­í¬ ìˆ˜", analysis_data.get('chunk_count', 0))

#             with tab2:
#                 keywords = analysis_data.get('keywords', [])
#                 if keywords:
#                     import pandas as pd
#                     df = pd.DataFrame(keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
#                     st.dataframe(df, use_container_width=True)
#                     st.bar_chart(df.set_index("í‚¤ì›Œë“œ"))
#                 else:
#                     st.info("í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

#             with tab3:
#                 st.text_area("ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸° (ì²« 500ì)", analysis_data.get('preview', ''), height=300)

#             with st.expander("ğŸ”§ ì²˜ë¦¬ ë¡œê·¸"):
#                 for i, msg in enumerate(result.get("messages", []), 1):
#                     st.text(f"{i}. {msg.content}")

#             with st.expander("ğŸ› ë””ë²„ê·¸ ì •ë³´"):
#                 st.json({
#                     "íŒŒì¼ëª…": uploaded_files2.name,
#                     "íŒŒì¼í¬ê¸°": f"{uploaded_files2.size:,} bytes",
#                     "ë©”ì‹œì§€ ìˆ˜": len(result.get("messages", []))
#                 })

#     except Exception as e:
#         st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
#     finally:
#         if os.path.exists(tmp_path):
#             os.unlink(tmp_path)




# --- Streamlit ì•± ì„¤ì • ---
st.set_page_config(page_title="AI ë„ìš°ë¯¸", page_icon="ğŸ’¬", layout="wide")

st.title("ğŸ’¬ ê³ ì„±êµ°ì²­ :blue[AI] ë„ìš°ë¯¸")


# --- í™”ë©´ ë””ìì¸ ---
st.markdown("""
    <style>
    /* ê¸°ë³¸ ë°”ë”” í°íŠ¸ ë° ë°°ê²½ */
    body {
        background-color: #f0f2f6;
        font-family: 'Noto Sans KR', sans-serif;
        color: #333;
    }

    
     /* ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ */
    .stChatInput input {
        border: 2px solid #3b82f6;
        border-radius: 25px;
        padding: 15px 25px;
        font-size: 16px;
        background: linear-gradient(to right, #f0f9ff, #ffffff);
        transition: all 0.3s ease;
    }
    
    .stChatInput input:focus {
        border-color: #2563eb;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
        background: white;
    }
    
    .stChatInput button {
        background: linear-gradient(135deg, #3b82f6, #2563eb);
        border-radius: 50%;
        transition: transform 0.3s ease;
    }
    
    .stChatInput button:hover {
        transform: scale(1.1) rotate(15deg);
    }

    </style>
""", unsafe_allow_html=True)

animated_input_css = """
    <style>
    /* ì…ë ¥ì°½ ë“±ì¥ ì• ë‹ˆë©”ì´ì…˜ */
    .stChatInput {
        animation: slide-up 0.5s ease-out;
    }
    
    @keyframes slide-up {
        from {
            opacity: 0;
            transform: translateY(50px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    /* íƒ€ì´í•‘ íš¨ê³¼ */
    .stChatInput input:focus {
        animation: typing-glow 2s ease-in-out infinite;
    }
    
    @keyframes typing-glow {
        0%, 100% { box-shadow: 0 0 5px rgba(59, 130, 246, 0.3); }
        50% { box-shadow: 0 0 20px rgba(59, 130, 246, 0.6); }
    }
    
    /* ë²„íŠ¼ íšŒì „ íš¨ê³¼ */
    .stChatInput button:hover {
        animation: rotate 0.5s ease;
    }
    
    @keyframes rotate {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }
    </style>
"""

st.markdown(animated_input_css, unsafe_allow_html=True)

with st.sidebar:
    # ë¡œê³ /íƒ€ì´í‹€ (ì„ íƒì‚¬í•­)
    st.markdown("""
        <div style="text-align: center; padding: 0rem;">
            <h1 style="font-size: 3.5rem; margin: 0; color: #1e293b; display: inline-block; vertical-align: middle;">ğŸ¤–</h1>
            <p style="font-size: 2.2rem; color: #1e748b; margin: 0 4rem 0 0; display: inline-block; vertical-align: middle;">
                AI í•™ìŠµê¸°
            </p>
        </div>
    """, unsafe_allow_html=True)
    
    # ========== ì„¹ì…˜ 1: ë¬¸ì„œ í•™ìŠµê¸° ==========
    st.markdown('<div class="sidebar-box">', unsafe_allow_html=True)
    
    st.markdown("""
        <div class="sidebar-header">
            <span>ğŸ“š</span>
            <span>ë¬¸ì„œ í•™ìŠµê¸°</span>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
        <p class="upload-label">
            ğŸ“ PDF íŒŒì¼ ì—…ë¡œë“œ 
            <span class="badge">ìµœëŒ€ 3ê°œ</span>
        </p>
    """, unsafe_allow_html=True)
    
    uploaded_files1 = st.file_uploader(
        "í•™ìŠµí•  PDF ì„ íƒ",
        type=['pdf'],
        accept_multiple_files=True,
        key="uploader1",
        label_visibility="collapsed"
    )
    
    # ì—…ë¡œë“œëœ íŒŒì¼ í‘œì‹œ
    if uploaded_files1:
        st.markdown("""
            <div style="background: #f0fdf4; padding: 0.5rem; border-radius: 8px; margin-top: 0.5rem;">
                <p style="margin: 0; font-size: 0.85rem; color: #15803d; font-weight: 500;">
                    âœ… {}ê°œ íŒŒì¼ ì„ íƒë¨
                </p>
            </div>
        """.format(len(uploaded_files1)), unsafe_allow_html=True)
        
        for i, file in enumerate(uploaded_files1[:3], 1):
            st.markdown(f"""
                <div style="font-size: 0.8rem; color: #475569; padding: 0.2rem 0.5rem;">
                    {i}. {file.name[:30]}{'...' if len(file.name) > 30 else ''}
                </div>
            """, unsafe_allow_html=True)
    
    process1 = st.button(
        "ğŸš€ í•™ìŠµ ì‹œì‘",
        key="process1",
        type="primary",
        # disabled=(uploaded_files1 is None or len(uploaded_files1) == 0),
        use_container_width=True
    )
    
    # ì‚¬ìš©ë°©ë²•
    st.markdown("""
        <div class="usage-box">
            <div class="usage-title">
                ğŸ’¡ ì‚¬ìš©ë°©ë²•
            </div>
            <ol class="usage-list">
                <li>PDF íŒŒì¼ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì—…ë¡œë“œ</li>
                <li>"í•™ìŠµ ì‹œì‘" ë²„íŠ¼ í´ë¦­</li>
                <li>í•™ìŠµ ì™„ë£Œ í›„ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ê°€ëŠ¥</li>
            </ol>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # êµ¬ë¶„ì„ 
    st.markdown('<hr class="custom-divider">', unsafe_allow_html=True)
    
    # ========== ì„¹ì…˜ 2: PDF ë¶„ì„ê¸° ==========
    st.markdown('<div class="sidebar-box">', unsafe_allow_html=True)
    
    st.markdown("""
        <div class="sidebar-header">
            <span>ğŸ”</span>
            <span>PDF ë¶„ì„ê¸°</span>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
        <p class="upload-label">
            ğŸ“ ë¶„ì„í•  PDF ì—…ë¡œë“œ
            <span class="badge badge-blue">1ê°œ</span>
        </p>
    """, unsafe_allow_html=True)
    
    uploaded_files2 = st.file_uploader(
        "ë¶„ì„í•  PDF ì„ íƒ",
        type=['pdf'],
        key="uploader2",
        label_visibility="collapsed"
    )
    
    # ì—…ë¡œë“œëœ íŒŒì¼ í‘œì‹œ
    if uploaded_files2:
        st.markdown(f"""
            <div style="background: #eff6ff; padding: 0.5rem; border-radius: 8px; margin-top: 0.5rem;">
                <p style="margin: 0; font-size: 0.85rem; color: #1e40af; font-weight: 500;">
                    ğŸ“„ {uploaded_files2.name[:35]}{'...' if len(uploaded_files2.name) > 35 else ''}
                </p>
                <p style="margin: 0.3rem 0 0 0; font-size: 0.75rem; color: #64748b;">
                    í¬ê¸°: {uploaded_files2.size / 1024:.1f} KB
                </p>
            </div>
        """, unsafe_allow_html=True)
    
    process2 = st.button(
        "ğŸš€ ë¶„ì„ ì‹œì‘",
        key="process2",
        # type="primary",
        # disabled=(uploaded_files2 is None),
        use_container_width=True
    )
    
    # ì‚¬ìš©ë°©ë²•
    st.markdown("""
        <div class="usage-box">
            <div class="usage-title">
                ğŸ’¡ ì‚¬ìš©ë°©ë²•
            </div>
            <ol class="usage-list">
                <li>PDF íŒŒì¼ 1ê°œ ì—…ë¡œë“œ</li>
                <li>"ë¶„ì„ ì‹œì‘" ë²„íŠ¼ í´ë¦­</li>
                <li>í‚¤ì›Œë“œ, í†µê³„ ë“± ë¶„ì„ ê²°ê³¼ í™•ì¸</li>
            </ol>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # êµ¬ë¶„ì„ 
    st.markdown('<hr class="custom-divider">', unsafe_allow_html=True)
    
    # ========== í•˜ë‹¨ ì •ë³´ ==========
    st.markdown("""
        <div style="text-align: center; padding: 1rem; color: #94a3b8; font-size: 0.8rem;">
            <p style="margin: 0;">Made with â¤ï¸ by ì •ë³´ê´€ë¦¬ Team</p>
            <p style="margin: 0.5rem 0 0 0;">v1.0.0 | 2025</p>
        </div>
    """, unsafe_allow_html=True)



# ë¬¸ì„œ í•™ìŠµ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
# if process1:
#     st.session_state["vectorstore"] = process1_f(uploaded_files1)

# # ë¬¸ì„œ ë¶„ì„ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
# if process2:
#     process2_f(uploaded_files2)

   

# ìŠ¤íŠ¸ë¦¼ë¦¿ session_stateì— ë©”ì‹œì§€ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        SystemMessage("ì €ëŠ” ê³ ì„±êµ°ì²­ ì§ì›ì„ ìœ„í•´ ìµœì„ ì„ ë‹¤í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "),  
        AIMessage("ë¬´ì—‡ì„ ë„ì™€ ë“œë¦´ê¹Œìš”?")
    ]

if "vectorstore" not in st.session_state:
    st.session_state["vectorstore"] = None

# ìŠ¤íŠ¸ë¦¼ë¦¿ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
for msg in st.session_state.messages:
    if msg.content:
        if isinstance(msg, SystemMessage):
            st.chat_message("system").write(msg.content)
        elif isinstance(msg, AIMessage):
            st.chat_message("assistant").write(msg.content)
        elif isinstance(msg, HumanMessage):
            st.chat_message("user").write(msg.content)
        # elif isinstance(msg, ToolMessage):
        #     st.chat_message("tool").write(msg.content)


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input(placeholder="âœ¨ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”?"):
    st.chat_message("user").write(prompt)
    st.session_state.messages.append(HumanMessage(prompt))

    # vectorstore ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if st.session_state.get("vectorstore") is not None:
        # ë²¡í„°ìŠ¤í† ì–´ ê¸°ë°˜ ë‹µë³€
        st.write("ğŸ“š í•™ìŠµëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤...")
        answer = get_ai_response(prompt)
        
        if answer == "ì£„ì†¡í•©ë‹ˆë‹¤. ":
            st.write("ğŸ¤– ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤...")
            response = get_ai_response(st.session_state["messages"])
            result = st.chat_message("assistant").write_stream(response)
            st.session_state["messages"].append(AIMessage(result)) 
        else:    
            st.chat_message("assistant").write(answer)
            st.session_state.messages.append(AIMessage(answer))
    else:
        # ê¸°ì¡´ ë„êµ¬ ê²°í•© LLM ë‹µë³€
        st.write("ğŸ¤– ì¼ë°˜ AI ëª¨ë“œë¡œ ë‹µë³€í•©ë‹ˆë‹¤...")
        response = get_ai_response(st.session_state["messages"])
        result = st.chat_message("assistant").write_stream(response)
        st.session_state["messages"].append(AIMessage(result)) 



